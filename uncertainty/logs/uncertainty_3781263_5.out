Starting job 3781269
SLURM assigned me the node(s): gpusrv10
Experiments are running under the following process IDs:
Experiment ID: 6	Process ID: 10330

During startup - Warning messages:
1: package ‘methods’ was built under R version 3.6.3 
2: package ‘datasets’ was built under R version 3.6.3 
3: package ‘utils’ was built under R version 3.6.3 
4: package ‘grDevices’ was built under R version 3.6.3 
5: package ‘graphics’ was built under R version 3.6.3 
6: package ‘stats’ was built under R version 3.6.3 
2021-10-26 10:57:03 (INFO): Running command 'run'
2021-10-26 10:57:03 (INFO): Started run with ID "6"
2021-10-26 10:57:16 (INFO): Data loaded succesfully
2021-10-26 10:57:16 (INFO): Model instantiated
Embedding dictionary:
 	Num conditions: 8
 	Embedding dim: 10
Encoder Architecture:
	Input Layer in, out and cond: 4000 128 0
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Mean/Var Layer in/out: 128 25
Decoder Architecture:
	First Layer in, out and cond:  25 128 10
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Output Layer in/out:  128 4000 

 |--------------------| 1.0%  - val_loss: 1203.6163662997 - val_trvae_loss: 1203.6163662997 |--------------------| 2.0%  - val_loss: 1160.6628972834 - val_trvae_loss: 1160.6628972834 |--------------------| 3.0%  - val_loss: 1127.3039217862 - val_trvae_loss: 1127.3039217862 |--------------------| 4.0%  - val_loss: 1112.7992165305 - val_trvae_loss: 1112.7992165305 |█-------------------| 5.0%  - val_loss: 1103.5127951882 - val_trvae_loss: 1103.5127951882 |█-------------------| 6.0%  - val_loss: 1095.9811789773 - val_trvae_loss: 1095.9811789773 |█-------------------| 7.0%  - val_loss: 1089.4289883700 - val_trvae_loss: 1089.4289883700 |█-------------------| 8.0%  - val_loss: 1083.7050004439 - val_trvae_loss: 1083.7050004439 |█-------------------| 9.0%  - val_loss: 1079.4100674716 - val_trvae_loss: 1079.4100674716 |██------------------| 10.0%  - val_loss: 1074.6821621982 - val_trvae_loss: 1074.6821621982 |██------------------| 11.0%  - val_loss: 1072.0562355735 - val_trvae_loss: 1072.0562355735 |██------------------| 12.0%  - val_loss: 1069.1432717063 - val_trvae_loss: 1069.1432717063 |██------------------| 13.0%  - val_loss: 1066.2502219460 - val_trvae_loss: 1066.2502219460 |██------------------| 14.0%  - val_loss: 1064.4788374467 - val_trvae_loss: 1064.4788374467 |███-----------------| 15.0%  - val_loss: 1062.1482987837 - val_trvae_loss: 1062.1482987837 |███-----------------| 16.0%  - val_loss: 1060.6369961825 - val_trvae_loss: 1060.6369961825 |███-----------------| 17.0%  - val_loss: 1058.9327503551 - val_trvae_loss: 1058.9327503551 |███-----------------| 18.0%  - val_loss: 1057.2162919478 - val_trvae_loss: 1057.2162919478 |███-----------------| 19.0%  - val_loss: 1056.8813143643 - val_trvae_loss: 1056.8813143643 |████----------------| 20.0%  - val_loss: 1054.9772172408 - val_trvae_loss: 1054.9772172408 |████----------------| 21.0%  - val_loss: 1053.3053644354 - val_trvae_loss: 1053.3053644354 |████----------------| 22.0%  - val_loss: 1053.2235939719 - val_trvae_loss: 1053.2235939719 |████----------------| 23.0%  - val_loss: 1052.6907903498 - val_trvae_loss: 1052.6907903498 |████----------------| 24.0%  - val_loss: 1051.2947443182 - val_trvae_loss: 1051.2947443182 |█████---------------| 25.0%  - val_loss: 1051.2567915483 - val_trvae_loss: 1051.2567915483 |█████---------------| 26.0%  - val_loss: 1050.7594160600 - val_trvae_loss: 1050.7594160600 |█████---------------| 27.0%  - val_loss: 1049.7968750000 - val_trvae_loss: 1049.7968750000 |█████---------------| 28.0%  - val_loss: 1048.7698863636 - val_trvae_loss: 1048.7698863636 |█████---------------| 29.0%  - val_loss: 1048.7046175870 - val_trvae_loss: 1048.7046175870 |██████--------------| 30.0%  - val_loss: 1048.1685513583 - val_trvae_loss: 1048.1685513583 |██████--------------| 31.0%  - val_loss: 1048.6908513849 - val_trvae_loss: 1048.6908513849 |██████--------------| 32.0%  - val_loss: 1047.8812755238 - val_trvae_loss: 1047.8812755238 |██████--------------| 33.0%  - val_loss: 1047.4402243874 - val_trvae_loss: 1047.4402243874 |██████--------------| 34.0%  - val_loss: 1046.6016346325 - val_trvae_loss: 1046.6016346325 |███████-------------| 35.0%  - val_loss: 1047.1530428800 - val_trvae_loss: 1047.1530428800 |███████-------------| 36.0%  - val_loss: 1046.4322509766 - val_trvae_loss: 1046.4322509766 |███████-------------| 37.0%  - val_loss: 1046.5659790039 - val_trvae_loss: 1046.5659790039 |███████-------------| 38.0%  - val_loss: 1046.4717684659 - val_trvae_loss: 1046.4717684659 |███████-------------| 39.0%  - val_loss: 1045.7878528942 - val_trvae_loss: 1045.7878528942 |████████------------| 40.0%  - val_loss: 1045.7931296609 - val_trvae_loss: 1045.7931296609 |████████------------| 41.0%  - val_loss: 1045.5642200817 - val_trvae_loss: 1045.5642200817 |████████------------| 42.0%  - val_loss: 1044.9259920987 - val_trvae_loss: 1044.9259920987 |████████------------| 43.0%  - val_loss: 1044.3610174006 - val_trvae_loss: 1044.3610174006 |████████------------| 44.0%  - val_loss: 1045.0171619762 - val_trvae_loss: 1045.0171619762 |█████████-----------| 45.0%  - val_loss: 1044.8111849698 - val_trvae_loss: 1044.8111849698 |█████████-----------| 46.0%  - val_loss: 1044.1959727894 - val_trvae_loss: 1044.1959727894 |█████████-----------| 47.0%  - val_loss: 1044.6473888050 - val_trvae_loss: 1044.6473888050 |█████████-----------| 48.0%  - val_loss: 1044.3848488548 - val_trvae_loss: 1044.3848488548 |█████████-----------| 49.0%  - val_loss: 1044.1995294744 - val_trvae_loss: 1044.1995294744 |██████████----------| 50.0%  - val_loss: 1044.3971113725 - val_trvae_loss: 1044.3971113725 |██████████----------| 51.0%  - val_loss: 1044.6292613636 - val_trvae_loss: 1044.6292613636 |██████████----------| 52.0%  - val_loss: 1044.4502286044 - val_trvae_loss: 1044.4502286044 |██████████----------| 53.0%  - val_loss: 1043.5885786577 - val_trvae_loss: 1043.5885786577 |██████████----------| 54.0%  - val_loss: 1044.4477705522 - val_trvae_loss: 1044.4477705522 |███████████---------| 55.0%  - val_loss: 1044.2168024237 - val_trvae_loss: 1044.2168024237 |███████████---------| 56.0%  - val_loss: 1044.1916836825 - val_trvae_loss: 1044.1916836825 |███████████---------| 57.0%  - val_loss: 1043.2387084961 - val_trvae_loss: 1043.2387084961 |███████████---------| 58.0%  - val_loss: 1043.5552701083 - val_trvae_loss: 1043.5552701083 |███████████---------| 59.0%  - val_loss: 1043.1708817915 - val_trvae_loss: 1043.1708817915 |████████████--------| 60.0%  - val_loss: 1043.4724398526 - val_trvae_loss: 1043.4724398526 |████████████--------| 61.0%  - val_loss: 1043.7749467330 - val_trvae_loss: 1043.7749467330 |████████████--------| 62.0%  - val_loss: 1043.5586603338 - val_trvae_loss: 1043.5586603338 |████████████--------| 63.0%  - val_loss: 1043.2536843040 - val_trvae_loss: 1043.2536843040 |████████████--------| 64.0%  - val_loss: 1043.6128983931 - val_trvae_loss: 1043.6128983931 |█████████████-------| 65.0%  - val_loss: 1043.5276322798 - val_trvae_loss: 1043.5276322798 |█████████████-------| 66.0%  - val_loss: 1043.1297385476 - val_trvae_loss: 1043.1297385476 |█████████████-------| 67.0%  - val_loss: 1042.8142755682 - val_trvae_loss: 1042.8142755682 |█████████████-------| 68.0%  - val_loss: 1043.5657015714 - val_trvae_loss: 1043.5657015714 |█████████████-------| 69.0%  - val_loss: 1042.7174016779 - val_trvae_loss: 1042.7174016779 |██████████████------| 70.0%  - val_loss: 1042.6736117276 - val_trvae_loss: 1042.6736117276 |██████████████------| 71.0%  - val_loss: 1043.2234275124 - val_trvae_loss: 1043.2234275124 |██████████████------| 72.0%  - val_loss: 1042.8472678445 - val_trvae_loss: 1042.8472678445 |██████████████------| 73.0%  - val_loss: 1042.7126353871 - val_trvae_loss: 1042.7126353871 |██████████████------| 74.0%  - val_loss: 1043.1737282493 - val_trvae_loss: 1043.1737282493 |███████████████-----| 75.0%  - val_loss: 1042.6345769709 - val_trvae_loss: 1042.6345769709 |███████████████-----| 76.0%  - val_loss: 1042.4732000178 - val_trvae_loss: 1042.4732000178 |███████████████-----| 77.0%  - val_loss: 1043.2783369585 - val_trvae_loss: 1043.2783369585 |███████████████-----| 78.0%  - val_loss: 1043.1173928001 - val_trvae_loss: 1043.1173928001 |███████████████-----| 79.0%  - val_loss: 1042.0468084162 - val_trvae_loss: 1042.0468084162 |████████████████----| 80.0%  - val_loss: 1042.8917680220 - val_trvae_loss: 1042.8917680220 |████████████████----| 81.0%  - val_loss: 1048.4777776545 - val_trvae_loss: 1044.9898293235 - val_landmark_loss: 3.4879480384 - val_labeled_loss: 3.4879480384 |████████████████----| 82.0%  - val_loss: 1046.6623979048 - val_trvae_loss: 1044.0194369229 - val_landmark_loss: 2.6429614371 - val_labeled_loss: 2.6429614371 |████████████████----| 83.0%  - val_loss: 1045.9504228072 - val_trvae_loss: 1043.3900035511 - val_landmark_loss: 2.5604288361 - val_labeled_loss: 2.5604288361 |████████████████----| 84.0%  - val_loss: 1045.9999722567 - val_trvae_loss: 1043.7555930398 - val_landmark_loss: 2.2443800840 - val_labeled_loss: 2.2443800840 |█████████████████---| 85.0%  - val_loss: 1045.8063520952 - val_trvae_loss: 1043.8353992809 - val_landmark_loss: 1.9709525217 - val_labeled_loss: 1.9709525217 |█████████████████---| 86.0%  - val_loss: 1046.1658269709 - val_trvae_loss: 1044.1603781960 - val_landmark_loss: 2.0054478212 - val_labeled_loss: 2.0054478212 |█████████████████---| 87.0%  - val_loss: 1045.0110973011 - val_trvae_loss: 1043.4084639116 - val_landmark_loss: 1.6026283177 - val_labeled_loss: 1.6026283177 |█████████████████---| 88.0%  - val_loss: 1045.0765103427 - val_trvae_loss: 1043.3100918857 - val_landmark_loss: 1.7664119926 - val_labeled_loss: 1.7664119926 |█████████████████---| 89.0%  - val_loss: 1045.2381369851 - val_trvae_loss: 1043.7242931019 - val_landmark_loss: 1.5138534037 - val_labeled_loss: 1.5138534037 |██████████████████--| 90.0%  - val_loss: 1044.5939497514 - val_trvae_loss: 1043.1316084428 - val_landmark_loss: 1.4623360905 - val_labeled_loss: 1.4623360905 |██████████████████--| 91.0%  - val_loss: 1044.9350197532 - val_trvae_loss: 1043.4901012074 - val_landmark_loss: 1.4449057091 - val_labeled_loss: 1.4449057091 |██████████████████--| 92.0%  - val_loss: 1044.1288119229 - val_trvae_loss: 1042.8143421520 - val_landmark_loss: 1.3144978014 - val_labeled_loss: 1.3144978014 |██████████████████--| 93.0%  - val_loss: 1044.3079279119 - val_trvae_loss: 1043.1211658825 - val_landmark_loss: 1.1867895181 - val_labeled_loss: 1.1867895181 |██████████████████--| 94.0%  - val_loss: 1044.3628595526 - val_trvae_loss: 1043.2401844371 - val_landmark_loss: 1.1226752996 - val_labeled_loss: 1.1226752996 |███████████████████-| 95.0%  - val_loss: 1044.4140125621 - val_trvae_loss: 1043.2620738636 - val_landmark_loss: 1.1519544558 - val_labeled_loss: 1.1519544558 |███████████████████-| 96.0%  - val_loss: 1044.6870061701 - val_trvae_loss: 1043.4433038885 - val_landmark_loss: 1.2437046170 - val_labeled_loss: 1.2437046170 |███████████████████-| 97.0%  - val_loss: 1044.4513216886 - val_trvae_loss: 1043.3779851740 - val_landmark_loss: 1.0733348131 - val_labeled_loss: 1.0733348131 |███████████████████-| 98.0%  - val_loss: 1045.5013538707 - val_trvae_loss: 1043.9978915128 - val_landmark_loss: 1.5034707243 - val_labeled_loss: 1.5034707243 |███████████████████-| 99.0%  - val_loss: 1043.9514770508 - val_trvae_loss: 1042.9243164062 - val_landmark_loss: 1.0271760009 - val_labeled_loss: 1.0271760009 |████████████████████| 100.0%  - val_loss: 1045.2431696112 - val_trvae_loss: 1043.1294500178 - val_landmark_loss: 2.1137057055 - val_labeled_loss: 2.1137057055
2021-10-26 10:59:27 (INFO): Model trained and saved, initiate surgery
Saving best state of network...
Best State was in Epoch 98
AnnData object with n_obs × n_vars = 2394 × 4000
    obs: 'study', 'cell_type', 'pred_label', 'pred_score'
    obsm: 'X_seurat', 'X_symphony'
Embedding dictionary:
 	Num conditions: 9
 	Embedding dim: 10
Encoder Architecture:
	Input Layer in, out and cond: 4000 128 0
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Mean/Var Layer in/out: 128 25
Decoder Architecture:
	First Layer in, out and cond:  25 128 10
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Output Layer in/out:  128 4000 

 |--------------------| 1.0%  - val_loss: 2032.4283447266 - val_trvae_loss: 2032.4283447266 |--------------------| 2.0%  - val_loss: 2032.2996826172 - val_trvae_loss: 2032.2996826172 |--------------------| 3.0%  - val_loss: 2030.2849731445 - val_trvae_loss: 2030.2849731445 |--------------------| 4.0%  - val_loss: 2029.0285644531 - val_trvae_loss: 2029.0285644531 |█-------------------| 5.0%  - val_loss: 2024.0069580078 - val_trvae_loss: 2024.0069580078 |█-------------------| 6.0%  - val_loss: 2023.9930419922 - val_trvae_loss: 2023.9930419922 |█-------------------| 7.0%  - val_loss: 2019.4976196289 - val_trvae_loss: 2019.4976196289 |█-------------------| 8.0%  - val_loss: 2018.1478271484 - val_trvae_loss: 2018.1478271484 |█-------------------| 9.0%  - val_loss: 2017.2532958984 - val_trvae_loss: 2017.2532958984 |██------------------| 10.0%  - val_loss: 2015.1843872070 - val_trvae_loss: 2015.1843872070 |██------------------| 11.0%  - val_loss: 2011.8297729492 - val_trvae_loss: 2011.8297729492 |██------------------| 12.0%  - val_loss: 2009.8880004883 - val_trvae_loss: 2009.8880004883 |██------------------| 13.0%  - val_loss: 2009.0670166016 - val_trvae_loss: 2009.0670166016 |██------------------| 14.0%  - val_loss: 2005.6942749023 - val_trvae_loss: 2005.6942749023 |███-----------------| 15.0%  - val_loss: 2005.6891479492 - val_trvae_loss: 2005.6891479492 |███-----------------| 16.0%  - val_loss: 2002.5756835938 - val_trvae_loss: 2002.5756835938 |███-----------------| 17.0%  - val_loss: 2003.7465209961 - val_trvae_loss: 2003.7465209961 |███-----------------| 18.0%  - val_loss: 2000.3280029297 - val_trvae_loss: 2000.3280029297 |███-----------------| 19.0%  - val_loss: 1997.0339355469 - val_trvae_loss: 1997.0339355469 |████----------------| 20.0%  - val_loss: 1996.6702880859 - val_trvae_loss: 1996.6702880859 |████----------------| 21.0%  - val_loss: 1993.5372924805 - val_trvae_loss: 1993.5372924805 |████----------------| 22.0%  - val_loss: 1992.2490234375 - val_trvae_loss: 1992.2490234375 |████----------------| 23.0%  - val_loss: 1991.0167846680 - val_trvae_loss: 1991.0167846680 |████----------------| 24.0%  - val_loss: 1989.7182006836 - val_trvae_loss: 1989.7182006836 |█████---------------| 25.0%  - val_loss: 1988.4888305664 - val_trvae_loss: 1988.4888305664 |█████---------------| 26.0%  - val_loss: 1987.5264892578 - val_trvae_loss: 1987.5264892578 |█████---------------| 27.0%  - val_loss: 1987.3178100586 - val_trvae_loss: 1987.3178100586 |█████---------------| 28.0%  - val_loss: 1985.8781738281 - val_trvae_loss: 1985.8781738281 |█████---------------| 29.0%  - val_loss: 1982.2144165039 - val_trvae_loss: 1982.2144165039 |██████--------------| 30.0%  - val_loss: 1981.4559936523 - val_trvae_loss: 1981.4559936523 |██████--------------| 31.0%  - val_loss: 1979.1705932617 - val_trvae_loss: 1979.1705932617 |██████--------------| 32.0%  - val_loss: 1980.1005249023 - val_trvae_loss: 1980.1005249023 |██████--------------| 33.0%  - val_loss: 1976.2082519531 - val_trvae_loss: 1976.2082519531 |██████--------------| 34.0%  - val_loss: 1976.4432373047 - val_trvae_loss: 1976.4432373047 |███████-------------| 35.0%  - val_loss: 1974.5701293945 - val_trvae_loss: 1974.5701293945 |███████-------------| 36.0%  - val_loss: 1974.9602661133 - val_trvae_loss: 1974.9602661133 |███████-------------| 37.0%  - val_loss: 1971.3759155273 - val_trvae_loss: 1971.3759155273 |███████-------------| 38.0%  - val_loss: 1969.8247070312 - val_trvae_loss: 1969.8247070312 |███████-------------| 39.0%  - val_loss: 1970.9069824219 - val_trvae_loss: 1970.9069824219 |████████------------| 40.0%  - val_loss: 1969.5631713867 - val_trvae_loss: 1969.5631713867 |████████------------| 41.0%  - val_loss: 1967.2838745117 - val_trvae_loss: 1967.2838745117 |████████------------| 42.0%  - val_loss: 1968.6698608398 - val_trvae_loss: 1968.6698608398 |████████------------| 43.0%  - val_loss: 1965.5558471680 - val_trvae_loss: 1965.5558471680 |████████------------| 44.0%  - val_loss: 1966.0744628906 - val_trvae_loss: 1966.0744628906 |█████████-----------| 45.0%  - val_loss: 1963.0977783203 - val_trvae_loss: 1963.0977783203 |█████████-----------| 46.0%  - val_loss: 1962.3345336914 - val_trvae_loss: 1962.3345336914 |█████████-----------| 47.0%  - val_loss: 1959.8359375000 - val_trvae_loss: 1959.8359375000 |█████████-----------| 48.0%  - val_loss: 1961.8500976562 - val_trvae_loss: 1961.8500976562 |█████████-----------| 49.0%  - val_loss: 1960.5393676758 - val_trvae_loss: 1960.5393676758 |██████████----------| 50.0%  - val_loss: 1958.9913940430 - val_trvae_loss: 1958.9913940430 |██████████----------| 51.0%  - val_loss: 1958.0136718750 - val_trvae_loss: 1958.0136718750 |██████████----------| 52.0%  - val_loss: 1956.7577514648 - val_trvae_loss: 1956.7577514648 |██████████----------| 53.0%  - val_loss: 1958.5350952148 - val_trvae_loss: 1958.5350952148 |██████████----------| 54.0%  - val_loss: 1954.5410766602 - val_trvae_loss: 1954.5410766602 |███████████---------| 55.0%  - val_loss: 1954.2532958984 - val_trvae_loss: 1954.2532958984 |███████████---------| 56.0%  - val_loss: 1953.3998413086 - val_trvae_loss: 1953.3998413086 |███████████---------| 57.0%  - val_loss: 1951.9315185547 - val_trvae_loss: 1951.9315185547 |███████████---------| 58.0%  - val_loss: 1951.9440307617 - val_trvae_loss: 1951.9440307617 |███████████---------| 59.0%  - val_loss: 1951.9247436523 - val_trvae_loss: 1951.9247436523 |████████████--------| 60.0%  - val_loss: 1947.8111572266 - val_trvae_loss: 1947.8111572266 |████████████--------| 61.0%  - val_loss: 1949.8132324219 - val_trvae_loss: 1949.8132324219 |████████████--------| 62.0%  - val_loss: 1949.1713256836 - val_trvae_loss: 1949.1713256836 |████████████--------| 63.0%  - val_loss: 1946.4122314453 - val_trvae_loss: 1946.4122314453 |████████████--------| 64.0%  - val_loss: 1946.0778808594 - val_trvae_loss: 1946.0778808594 |█████████████-------| 65.0%  - val_loss: 1945.9789428711 - val_trvae_loss: 1945.9789428711 |█████████████-------| 66.0%  - val_loss: 1945.9348144531 - val_trvae_loss: 1945.9348144531 |█████████████-------| 67.0%  - val_loss: 1944.3166503906 - val_trvae_loss: 1944.3166503906 |█████████████-------| 68.0%  - val_loss: 1944.3676757812 - val_trvae_loss: 1944.3676757812 |█████████████-------| 69.0%  - val_loss: 1942.7139282227 - val_trvae_loss: 1942.7139282227 |██████████████------| 70.0%  - val_loss: 1942.5622558594 - val_trvae_loss: 1942.5622558594 |██████████████------| 71.0%  - val_loss: 1942.4428710938 - val_trvae_loss: 1942.4428710938 |██████████████------| 72.0%  - val_loss: 1940.8104248047 - val_trvae_loss: 1940.8104248047 |██████████████------| 73.0%  - val_loss: 1939.5051879883 - val_trvae_loss: 1939.5051879883 |██████████████------| 74.0%  - val_loss: 1939.9552001953 - val_trvae_loss: 1939.9552001953 |███████████████-----| 75.0%  - val_loss: 1940.1641845703 - val_trvae_loss: 1940.1641845703 |███████████████-----| 76.0%  - val_loss: 1940.3522949219 - val_trvae_loss: 1940.3522949219 |███████████████-----| 77.0%  - val_loss: 1936.3821411133 - val_trvae_loss: 1936.3821411133 |███████████████-----| 78.0%  - val_loss: 1936.8689575195 - val_trvae_loss: 1936.8689575195 |███████████████-----| 79.0%  - val_loss: 1936.5560302734 - val_trvae_loss: 1936.5560302734 |████████████████----| 80.0%  - val_loss: 1937.4680175781 - val_trvae_loss: 1937.4680175781
Initializing unlabeled landmarks with Leiden-Clustering with an unknown number of clusters.
Leiden Clustering succesful. Found 14 clusters.
 |████████████████----| 81.0%  - val_loss: 1935.7769775391 - val_trvae_loss: 1935.7748413086 - val_landmark_loss: 0.0021609557 - val_unlabeled_loss: 2.1609556675 |████████████████----| 82.0%  - val_loss: 1935.2189941406 - val_trvae_loss: 1935.2168579102 - val_landmark_loss: 0.0021414430 - val_unlabeled_loss: 2.1414428949 |████████████████----| 83.0%  - val_loss: 1936.8716430664 - val_trvae_loss: 1936.8692626953 - val_landmark_loss: 0.0023390510 - val_unlabeled_loss: 2.3390508890 |████████████████----| 84.0%  - val_loss: 1934.2893066406 - val_trvae_loss: 1934.2870483398 - val_landmark_loss: 0.0022417086 - val_unlabeled_loss: 2.2417084575 |█████████████████---| 85.0%  - val_loss: 1934.3810424805 - val_trvae_loss: 1934.3789062500 - val_landmark_loss: 0.0021706671 - val_unlabeled_loss: 2.1706669927 |█████████████████---| 86.0%  - val_loss: 1933.4566040039 - val_trvae_loss: 1933.4545288086 - val_landmark_loss: 0.0021096640 - val_unlabeled_loss: 2.1096639633 |█████████████████---| 87.0%  - val_loss: 1933.5871582031 - val_trvae_loss: 1933.5850219727 - val_landmark_loss: 0.0021313123 - val_unlabeled_loss: 2.1313121319 |█████████████████---| 88.0%  - val_loss: 1932.0747070312 - val_trvae_loss: 1932.0726318359 - val_landmark_loss: 0.0020759541 - val_unlabeled_loss: 2.0759540796 |█████████████████---| 89.0%  - val_loss: 1930.7022705078 - val_trvae_loss: 1930.7001342773 - val_landmark_loss: 0.0021378416 - val_unlabeled_loss: 2.1378414631 |██████████████████--| 90.0%  - val_loss: 1930.7417602539 - val_trvae_loss: 1930.7396850586 - val_landmark_loss: 0.0020531162 - val_unlabeled_loss: 2.0531160831 |██████████████████--| 91.0%  - val_loss: 1933.0060424805 - val_trvae_loss: 1933.0037841797 - val_landmark_loss: 0.0022055659 - val_unlabeled_loss: 2.2055658698 |██████████████████--| 92.0%  - val_loss: 1931.1773071289 - val_trvae_loss: 1931.1753540039 - val_landmark_loss: 0.0019753086 - val_unlabeled_loss: 1.9753084183 |██████████████████--| 93.0%  - val_loss: 1930.5452880859 - val_trvae_loss: 1930.5430297852 - val_landmark_loss: 0.0022476508 - val_unlabeled_loss: 2.2476506829 |██████████████████--| 94.0%  - val_loss: 1930.0529785156 - val_trvae_loss: 1930.0508422852 - val_landmark_loss: 0.0021315655 - val_unlabeled_loss: 2.1315652728 |███████████████████-| 95.0%  - val_loss: 1929.8500976562 - val_trvae_loss: 1929.8478393555 - val_landmark_loss: 0.0022753547 - val_unlabeled_loss: 2.2753546238 |███████████████████-| 96.0%  - val_loss: 1928.1736450195 - val_trvae_loss: 1928.1713867188 - val_landmark_loss: 0.0022651137 - val_unlabeled_loss: 2.2651135325 |███████████████████-| 97.0%  - val_loss: 1928.7161254883 - val_trvae_loss: 1928.7139282227 - val_landmark_loss: 0.0022068507 - val_unlabeled_loss: 2.2068506479 |███████████████████-| 98.0%  - val_loss: 1928.6946411133 - val_trvae_loss: 1928.6925048828 - val_landmark_loss: 0.0021247563 - val_unlabeled_loss: 2.1247562170 |███████████████████-| 99.0%  - val_loss: 1927.5008544922 - val_trvae_loss: 1927.4986572266 - val_landmark_loss: 0.0022156682 - val_unlabeled_loss: 2.2156680822 |████████████████████| 100.0%  - val_loss: 1927.9888305664 - val_trvae_loss: 1927.9867553711 - val_landmark_loss: 0.0020407190 - val_unlabeled_loss: 2.0407188535
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
2021-10-26 10:59:51 (INFO): Computing metrics
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
2021-10-26 10:59:52 (INFO): Compute integration metrics
... storing 'study' as categorical
... storing 'cell_type' as categorical
slurmstepd: error: *** JOB 3781269 ON gpusrv10 CANCELLED AT 2021-10-26T11:01:37 ***
