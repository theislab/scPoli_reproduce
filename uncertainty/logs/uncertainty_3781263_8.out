Starting job 3781263
SLURM assigned me the node(s): gpusrv13
gpusrv11
gpusrv11
gpusrv10
gpusrv10
gpusrv09
gpusrv09
gpusrv12
gpusrv12
Experiments are running under the following process IDs:
Experiment ID: 9	Process ID: 11302

During startup - Warning messages:
1: package ‘methods’ was built under R version 3.6.3 
2: package ‘datasets’ was built under R version 3.6.3 
3: package ‘utils’ was built under R version 3.6.3 
4: package ‘grDevices’ was built under R version 3.6.3 
5: package ‘graphics’ was built under R version 3.6.3 
6: package ‘stats’ was built under R version 3.6.3 
2021-10-26 10:57:01 (INFO): Running command 'run'
2021-10-26 10:57:01 (INFO): Started run with ID "9"
2021-10-26 10:57:16 (INFO): Data loaded succesfully
2021-10-26 10:57:16 (INFO): Model instantiated
Embedding dictionary:
 	Num conditions: 8
 	Embedding dim: 10
Encoder Architecture:
	Input Layer in, out and cond: 4000 128 0
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Mean/Var Layer in/out: 128 25
Decoder Architecture:
	First Layer in, out and cond:  25 128 10
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Output Layer in/out:  128 4000 

 |--------------------| 1.0%  - val_loss: 1180.2904052734 - val_trvae_loss: 1180.2904052734 |--------------------| 2.0%  - val_loss: 1145.2946000533 - val_trvae_loss: 1145.2946000533 |--------------------| 3.0%  - val_loss: 1098.9900568182 - val_trvae_loss: 1098.9900568182 |--------------------| 4.0%  - val_loss: 1085.1323575107 - val_trvae_loss: 1085.1323575107 |█-------------------| 5.0%  - val_loss: 1076.5878462358 - val_trvae_loss: 1076.5878462358 |█-------------------| 6.0%  - val_loss: 1069.2789306641 - val_trvae_loss: 1069.2789306641 |█-------------------| 7.0%  - val_loss: 1063.0512473366 - val_trvae_loss: 1063.0512473366 |█-------------------| 8.0%  - val_loss: 1058.1954900568 - val_trvae_loss: 1058.1954900568 |█-------------------| 9.0%  - val_loss: 1054.5108753551 - val_trvae_loss: 1054.5108753551 |██------------------| 10.0%  - val_loss: 1050.7345248136 - val_trvae_loss: 1050.7345248136 |██------------------| 11.0%  - val_loss: 1048.3148970170 - val_trvae_loss: 1048.3148970170 |██------------------| 12.0%  - val_loss: 1045.1237848455 - val_trvae_loss: 1045.1237848455 |██------------------| 13.0%  - val_loss: 1042.9309747869 - val_trvae_loss: 1042.9309747869 |██------------------| 14.0%  - val_loss: 1040.7117198597 - val_trvae_loss: 1040.7117198597 |███-----------------| 15.0%  - val_loss: 1039.6652388139 - val_trvae_loss: 1039.6652388139 |███-----------------| 16.0%  - val_loss: 1037.8354325728 - val_trvae_loss: 1037.8354325728 |███-----------------| 17.0%  - val_loss: 1036.2974798029 - val_trvae_loss: 1036.2974798029 |███-----------------| 18.0%  - val_loss: 1035.5943270597 - val_trvae_loss: 1035.5943270597 |███-----------------| 19.0%  - val_loss: 1033.6811024059 - val_trvae_loss: 1033.6811024059 |████----------------| 20.0%  - val_loss: 1032.7883411754 - val_trvae_loss: 1032.7883411754 |████----------------| 21.0%  - val_loss: 1032.0911088423 - val_trvae_loss: 1032.0911088423 |████----------------| 22.0%  - val_loss: 1031.3058249734 - val_trvae_loss: 1031.3058249734 |████----------------| 23.0%  - val_loss: 1030.0856267756 - val_trvae_loss: 1030.0856267756 |████----------------| 24.0%  - val_loss: 1029.4408624822 - val_trvae_loss: 1029.4408624822 |█████---------------| 25.0%  - val_loss: 1028.6635409268 - val_trvae_loss: 1028.6635409268 |█████---------------| 26.0%  - val_loss: 1028.5905539773 - val_trvae_loss: 1028.5905539773 |█████---------------| 27.0%  - val_loss: 1028.0522017045 - val_trvae_loss: 1028.0522017045 |█████---------------| 28.0%  - val_loss: 1027.1581642844 - val_trvae_loss: 1027.1581642844 |█████---------------| 29.0%  - val_loss: 1027.0142433860 - val_trvae_loss: 1027.0142433860 |██████--------------| 30.0%  - val_loss: 1026.1158835671 - val_trvae_loss: 1026.1158835671 |██████--------------| 31.0%  - val_loss: 1026.4431263317 - val_trvae_loss: 1026.4431263317 |██████--------------| 32.0%  - val_loss: 1026.0627330433 - val_trvae_loss: 1026.0627330433 |██████--------------| 33.0%  - val_loss: 1025.0984552557 - val_trvae_loss: 1025.0984552557 |██████--------------| 34.0%  - val_loss: 1025.3312377930 - val_trvae_loss: 1025.3312377930 |███████-------------| 35.0%  - val_loss: 1024.7238991477 - val_trvae_loss: 1024.7238991477 |███████-------------| 36.0%  - val_loss: 1024.9156272195 - val_trvae_loss: 1024.9156272195 |███████-------------| 37.0%  - val_loss: 1024.3431840376 - val_trvae_loss: 1024.3431840376 |███████-------------| 38.0%  - val_loss: 1023.4522039240 - val_trvae_loss: 1023.4522039240 |███████-------------| 39.0%  - val_loss: 1023.8062355735 - val_trvae_loss: 1023.8062355735 |████████------------| 40.0%  - val_loss: 1023.4475763494 - val_trvae_loss: 1023.4475763494 |████████------------| 41.0%  - val_loss: 1022.7981178977 - val_trvae_loss: 1022.7981178977 |████████------------| 42.0%  - val_loss: 1023.0041448420 - val_trvae_loss: 1023.0041448420 |████████------------| 43.0%  - val_loss: 1022.6983975497 - val_trvae_loss: 1022.6983975497 |████████------------| 44.0%  - val_loss: 1022.7808449485 - val_trvae_loss: 1022.7808449485 |█████████-----------| 45.0%  - val_loss: 1022.8261441317 - val_trvae_loss: 1022.8261441317 |█████████-----------| 46.0%  - val_loss: 1022.3396939364 - val_trvae_loss: 1022.3396939364 |█████████-----------| 47.0%  - val_loss: 1022.1727183949 - val_trvae_loss: 1022.1727183949 |█████████-----------| 48.0%  - val_loss: 1022.0258678089 - val_trvae_loss: 1022.0258678089 |█████████-----------| 49.0%  - val_loss: 1022.2007335316 - val_trvae_loss: 1022.2007335316 |██████████----------| 50.0%  - val_loss: 1021.9684559215 - val_trvae_loss: 1021.9684559215 |██████████----------| 51.0%  - val_loss: 1021.6849143288 - val_trvae_loss: 1021.6849143288 |██████████----------| 52.0%  - val_loss: 1021.6415571733 - val_trvae_loss: 1021.6415571733 |██████████----------| 53.0%  - val_loss: 1021.2784645774 - val_trvae_loss: 1021.2784645774 |██████████----------| 54.0%  - val_loss: 1021.4422718395 - val_trvae_loss: 1021.4422718395 |███████████---------| 55.0%  - val_loss: 1021.0678766424 - val_trvae_loss: 1021.0678766424 |███████████---------| 56.0%  - val_loss: 1021.5578613281 - val_trvae_loss: 1021.5578613281 |███████████---------| 57.0%  - val_loss: 1020.8213889382 - val_trvae_loss: 1020.8213889382 |███████████---------| 58.0%  - val_loss: 1020.4713800604 - val_trvae_loss: 1020.4713800604 |███████████---------| 59.0%  - val_loss: 1020.7525190874 - val_trvae_loss: 1020.7525190874 |████████████--------| 60.0%  - val_loss: 1020.6324074485 - val_trvae_loss: 1020.6324074485 |████████████--------| 61.0%  - val_loss: 1020.8045987216 - val_trvae_loss: 1020.8045987216 |████████████--------| 62.0%  - val_loss: 1020.1771129261 - val_trvae_loss: 1020.1771129261 |████████████--------| 63.0%  - val_loss: 1020.4256647283 - val_trvae_loss: 1020.4256647283 |████████████--------| 64.0%  - val_loss: 1020.3345669833 - val_trvae_loss: 1020.3345669833 |█████████████-------| 65.0%  - val_loss: 1020.0546764027 - val_trvae_loss: 1020.0546764027 |█████████████-------| 66.0%  - val_loss: 1019.9266801314 - val_trvae_loss: 1019.9266801314 |█████████████-------| 67.0%  - val_loss: 1019.9626353871 - val_trvae_loss: 1019.9626353871 |█████████████-------| 68.0%  - val_loss: 1019.8155739524 - val_trvae_loss: 1019.8155739524 |█████████████-------| 69.0%  - val_loss: 1020.1282737038 - val_trvae_loss: 1020.1282737038 |██████████████------| 70.0%  - val_loss: 1019.9843472567 - val_trvae_loss: 1019.9843472567 |██████████████------| 71.0%  - val_loss: 1020.4681951349 - val_trvae_loss: 1020.4681951349 |██████████████------| 72.0%  - val_loss: 1019.7120472301 - val_trvae_loss: 1019.7120472301 |██████████████------| 73.0%  - val_loss: 1020.0167291815 - val_trvae_loss: 1020.0167291815 |██████████████------| 74.0%  - val_loss: 1020.0510975231 - val_trvae_loss: 1020.0510975231 |███████████████-----| 75.0%  - val_loss: 1020.0822420987 - val_trvae_loss: 1020.0822420987 |███████████████-----| 76.0%  - val_loss: 1019.4743652344 - val_trvae_loss: 1019.4743652344 |███████████████-----| 77.0%  - val_loss: 1019.6472112482 - val_trvae_loss: 1019.6472112482 |███████████████-----| 78.0%  - val_loss: 1019.4556829279 - val_trvae_loss: 1019.4556829279 |███████████████-----| 79.0%  - val_loss: 1019.9379938299 - val_trvae_loss: 1019.9379938299 |████████████████----| 80.0%  - val_loss: 1019.7533957741 - val_trvae_loss: 1019.7533957741 |████████████████----| 81.0%  - val_loss: 1025.2590221058 - val_trvae_loss: 1021.7120416815 - val_landmark_loss: 3.5469684818 - val_labeled_loss: 3.5469684818 |████████████████----| 82.0%  - val_loss: 1024.5944380327 - val_trvae_loss: 1021.7362726385 - val_landmark_loss: 2.8581576239 - val_labeled_loss: 2.8581576239 |████████████████----| 83.0%  - val_loss: 1023.6492087624 - val_trvae_loss: 1021.2638882724 - val_landmark_loss: 2.3853158626 - val_labeled_loss: 2.3853158626 |████████████████----| 84.0%  - val_loss: 1023.8685302734 - val_trvae_loss: 1021.5901711204 - val_landmark_loss: 2.2783614939 - val_labeled_loss: 2.2783614939 |█████████████████---| 85.0%  - val_loss: 1023.6411021839 - val_trvae_loss: 1021.3745061701 - val_landmark_loss: 2.2666031881 - val_labeled_loss: 2.2666031881 |█████████████████---| 86.0%  - val_loss: 1022.7456054688 - val_trvae_loss: 1020.9354469993 - val_landmark_loss: 1.8101560311 - val_labeled_loss: 1.8101560311 |█████████████████---| 87.0%  - val_loss: 1022.5226939808 - val_trvae_loss: 1020.7339533026 - val_landmark_loss: 1.7887312716 - val_labeled_loss: 1.7887312716 |█████████████████---| 88.0%  - val_loss: 1022.4300204190 - val_trvae_loss: 1020.7188276811 - val_landmark_loss: 1.7112043879 - val_labeled_loss: 1.7112043879 |█████████████████---| 89.0%  - val_loss: 1022.2696810636 - val_trvae_loss: 1020.6521550959 - val_landmark_loss: 1.6175191348 - val_labeled_loss: 1.6175191348 |██████████████████--| 90.0%  - val_loss: 1021.8576826616 - val_trvae_loss: 1020.4734885476 - val_landmark_loss: 1.3841905377 - val_labeled_loss: 1.3841905377 |██████████████████--| 91.0%  - val_loss: 1021.9037808505 - val_trvae_loss: 1020.3884776722 - val_landmark_loss: 1.5153070472 - val_labeled_loss: 1.5153070472 |██████████████████--| 92.0%  - val_loss: 1021.7531738281 - val_trvae_loss: 1020.4060557972 - val_landmark_loss: 1.3471144546 - val_labeled_loss: 1.3471144546 |██████████████████--| 93.0%  - val_loss: 1021.8133822354 - val_trvae_loss: 1020.5726040927 - val_landmark_loss: 1.2407751734 - val_labeled_loss: 1.2407751734 |██████████████████--| 94.0%  - val_loss: 1021.3921009411 - val_trvae_loss: 1020.3158902255 - val_landmark_loss: 1.0762004636 - val_labeled_loss: 1.0762004636 |███████████████████-| 95.0%  - val_loss: 1021.2685435902 - val_trvae_loss: 1020.1867453835 - val_landmark_loss: 1.0818001465 - val_labeled_loss: 1.0818001465 |███████████████████-| 96.0%  - val_loss: 1021.0349897905 - val_trvae_loss: 1020.0980945934 - val_landmark_loss: 0.9368910248 - val_labeled_loss: 0.9368910248 |███████████████████-| 97.0%  - val_loss: 1021.7667680220 - val_trvae_loss: 1020.6210105202 - val_landmark_loss: 1.1457743482 - val_labeled_loss: 1.1457743482 |███████████████████-| 98.0%  - val_loss: 1021.0790238814 - val_trvae_loss: 1020.1173706055 - val_landmark_loss: 0.9616639885 - val_labeled_loss: 0.9616639885 |███████████████████-| 99.0%  - val_loss: 1021.6452470259 - val_trvae_loss: 1020.4882867987 - val_landmark_loss: 1.1569589431 - val_labeled_loss: 1.1569589431 |████████████████████| 100.0%  - val_loss: 1020.7192105380 - val_trvae_loss: 1019.8022460938 - val_landmark_loss: 0.9169597842 - val_labeled_loss: 0.9169597842
2021-10-26 10:59:25 (INFO): Model trained and saved, initiate surgery
Saving best state of network...
Best State was in Epoch 99
AnnData object with n_obs × n_vars = 2285 × 4000
    obs: 'study', 'cell_type', 'pred_label', 'pred_score'
    obsm: 'X_seurat', 'X_symphony'
Embedding dictionary:
 	Num conditions: 9
 	Embedding dim: 10
Encoder Architecture:
	Input Layer in, out and cond: 4000 128 0
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Mean/Var Layer in/out: 128 25
Decoder Architecture:
	First Layer in, out and cond:  25 128 10
	Hidden Layer 1 in/out: 128 128
	Hidden Layer 2 in/out: 128 128
	Output Layer in/out:  128 4000 

 |--------------------| 1.0%  - val_loss: 2075.5737304688 - val_trvae_loss: 2075.5737304688 |--------------------| 2.0%  - val_loss: 2074.9533691406 - val_trvae_loss: 2074.9533691406 |--------------------| 3.0%  - val_loss: 2071.7573242188 - val_trvae_loss: 2071.7573242188 |--------------------| 4.0%  - val_loss: 2067.1230468750 - val_trvae_loss: 2067.1230468750 |█-------------------| 5.0%  - val_loss: 2065.5037841797 - val_trvae_loss: 2065.5037841797 |█-------------------| 6.0%  - val_loss: 2063.1057128906 - val_trvae_loss: 2063.1057128906 |█-------------------| 7.0%  - val_loss: 2062.2949829102 - val_trvae_loss: 2062.2949829102 |█-------------------| 8.0%  - val_loss: 2059.6429443359 - val_trvae_loss: 2059.6429443359 |█-------------------| 9.0%  - val_loss: 2055.0029907227 - val_trvae_loss: 2055.0029907227 |██------------------| 10.0%  - val_loss: 2055.9159545898 - val_trvae_loss: 2055.9159545898 |██------------------| 11.0%  - val_loss: 2052.1614990234 - val_trvae_loss: 2052.1614990234 |██------------------| 12.0%  - val_loss: 2051.8928833008 - val_trvae_loss: 2051.8928833008 |██------------------| 13.0%  - val_loss: 2048.5954589844 - val_trvae_loss: 2048.5954589844 |██------------------| 14.0%  - val_loss: 2045.2296142578 - val_trvae_loss: 2045.2296142578 |███-----------------| 15.0%  - val_loss: 2043.1640014648 - val_trvae_loss: 2043.1640014648 |███-----------------| 16.0%  - val_loss: 2043.3894042969 - val_trvae_loss: 2043.3894042969 |███-----------------| 17.0%  - val_loss: 2037.6743774414 - val_trvae_loss: 2037.6743774414 |███-----------------| 18.0%  - val_loss: 2035.6548461914 - val_trvae_loss: 2035.6548461914 |███-----------------| 19.0%  - val_loss: 2033.7153930664 - val_trvae_loss: 2033.7153930664 |████----------------| 20.0%  - val_loss: 2033.7495117188 - val_trvae_loss: 2033.7495117188 |████----------------| 21.0%  - val_loss: 2031.1576538086 - val_trvae_loss: 2031.1576538086 |████----------------| 22.0%  - val_loss: 2030.1127929688 - val_trvae_loss: 2030.1127929688 |████----------------| 23.0%  - val_loss: 2027.0714111328 - val_trvae_loss: 2027.0714111328 |████----------------| 24.0%  - val_loss: 2028.1469726562 - val_trvae_loss: 2028.1469726562 |█████---------------| 25.0%  - val_loss: 2024.8020629883 - val_trvae_loss: 2024.8020629883 |█████---------------| 26.0%  - val_loss: 2021.8170166016 - val_trvae_loss: 2021.8170166016 |█████---------------| 27.0%  - val_loss: 2019.7616577148 - val_trvae_loss: 2019.7616577148 |█████---------------| 28.0%  - val_loss: 2017.5340576172 - val_trvae_loss: 2017.5340576172 |█████---------------| 29.0%  - val_loss: 2020.5938110352 - val_trvae_loss: 2020.5938110352 |██████--------------| 30.0%  - val_loss: 2014.0252685547 - val_trvae_loss: 2014.0252685547 |██████--------------| 31.0%  - val_loss: 2013.2301025391 - val_trvae_loss: 2013.2301025391 |██████--------------| 32.0%  - val_loss: 2010.4034423828 - val_trvae_loss: 2010.4034423828 |██████--------------| 33.0%  - val_loss: 2010.0677490234 - val_trvae_loss: 2010.0677490234 |██████--------------| 34.0%  - val_loss: 2008.9724121094 - val_trvae_loss: 2008.9724121094 |███████-------------| 35.0%  - val_loss: 2007.3659667969 - val_trvae_loss: 2007.3659667969 |███████-------------| 36.0%  - val_loss: 2007.4580688477 - val_trvae_loss: 2007.4580688477 |███████-------------| 37.0%  - val_loss: 2003.6379394531 - val_trvae_loss: 2003.6379394531 |███████-------------| 38.0%  - val_loss: 2002.2127685547 - val_trvae_loss: 2002.2127685547 |███████-------------| 39.0%  - val_loss: 2001.1246948242 - val_trvae_loss: 2001.1246948242 |████████------------| 40.0%  - val_loss: 2002.4806518555 - val_trvae_loss: 2002.4806518555 |████████------------| 41.0%  - val_loss: 1998.8887329102 - val_trvae_loss: 1998.8887329102 |████████------------| 42.0%  - val_loss: 1998.4180297852 - val_trvae_loss: 1998.4180297852 |████████------------| 43.0%  - val_loss: 1995.5307006836 - val_trvae_loss: 1995.5307006836 |████████------------| 44.0%  - val_loss: 1995.7084350586 - val_trvae_loss: 1995.7084350586 |█████████-----------| 45.0%  - val_loss: 1995.5358276367 - val_trvae_loss: 1995.5358276367 |█████████-----------| 46.0%  - val_loss: 1996.6601562500 - val_trvae_loss: 1996.6601562500 |█████████-----------| 47.0%  - val_loss: 1994.0817871094 - val_trvae_loss: 1994.0817871094 |█████████-----------| 48.0%  - val_loss: 1991.8093872070 - val_trvae_loss: 1991.8093872070 |█████████-----------| 49.0%  - val_loss: 1989.9827270508 - val_trvae_loss: 1989.9827270508 |██████████----------| 50.0%  - val_loss: 1988.2233276367 - val_trvae_loss: 1988.2233276367 |██████████----------| 51.0%  - val_loss: 1985.7402343750 - val_trvae_loss: 1985.7402343750 |██████████----------| 52.0%  - val_loss: 1985.9780273438 - val_trvae_loss: 1985.9780273438 |██████████----------| 53.0%  - val_loss: 1985.9462280273 - val_trvae_loss: 1985.9462280273 |██████████----------| 54.0%  - val_loss: 1986.2600708008 - val_trvae_loss: 1986.2600708008 |███████████---------| 55.0%  - val_loss: 1983.3271484375 - val_trvae_loss: 1983.3271484375 |███████████---------| 56.0%  - val_loss: 1983.5639038086 - val_trvae_loss: 1983.5639038086 |███████████---------| 57.0%  - val_loss: 1982.7618408203 - val_trvae_loss: 1982.7618408203 |███████████---------| 58.0%  - val_loss: 1979.7041015625 - val_trvae_loss: 1979.7041015625 |███████████---------| 59.0%  - val_loss: 1979.8644409180 - val_trvae_loss: 1979.8644409180 |████████████--------| 60.0%  - val_loss: 1975.6738891602 - val_trvae_loss: 1975.6738891602 |████████████--------| 61.0%  - val_loss: 1974.3893432617 - val_trvae_loss: 1974.3893432617 |████████████--------| 62.0%  - val_loss: 1975.1830444336 - val_trvae_loss: 1975.1830444336 |████████████--------| 63.0%  - val_loss: 1974.3433837891 - val_trvae_loss: 1974.3433837891 |████████████--------| 64.0%  - val_loss: 1974.0121459961 - val_trvae_loss: 1974.0121459961 |█████████████-------| 65.0%  - val_loss: 1972.7808227539 - val_trvae_loss: 1972.7808227539 |█████████████-------| 66.0%  - val_loss: 1974.6416015625 - val_trvae_loss: 1974.6416015625 |█████████████-------| 67.0%  - val_loss: 1972.3983764648 - val_trvae_loss: 1972.3983764648 |█████████████-------| 68.0%  - val_loss: 1970.4111938477 - val_trvae_loss: 1970.4111938477 |█████████████-------| 69.0%  - val_loss: 1968.4894409180 - val_trvae_loss: 1968.4894409180 |██████████████------| 70.0%  - val_loss: 1966.8151245117 - val_trvae_loss: 1966.8151245117 |██████████████------| 71.0%  - val_loss: 1967.4873657227 - val_trvae_loss: 1967.4873657227 |██████████████------| 72.0%  - val_loss: 1968.8569335938 - val_trvae_loss: 1968.8569335938 |██████████████------| 73.0%  - val_loss: 1969.7570190430 - val_trvae_loss: 1969.7570190430 |██████████████------| 74.0%  - val_loss: 1964.8499145508 - val_trvae_loss: 1964.8499145508 |███████████████-----| 75.0%  - val_loss: 1963.8789672852 - val_trvae_loss: 1963.8789672852 |███████████████-----| 76.0%  - val_loss: 1962.8506469727 - val_trvae_loss: 1962.8506469727 |███████████████-----| 77.0%  - val_loss: 1966.4152832031 - val_trvae_loss: 1966.4152832031 |███████████████-----| 78.0%  - val_loss: 1963.1161499023 - val_trvae_loss: 1963.1161499023 |███████████████-----| 79.0%  - val_loss: 1962.1456909180 - val_trvae_loss: 1962.1456909180 |████████████████----| 80.0%  - val_loss: 1961.9967651367 - val_trvae_loss: 1961.9967651367
Initializing unlabeled landmarks with Leiden-Clustering with an unknown number of clusters.
Leiden Clustering succesful. Found 14 clusters.
 |████████████████----| 81.0%  - val_loss: 1960.1039428711 - val_trvae_loss: 1960.1000976562 - val_landmark_loss: 0.0038451501 - val_unlabeled_loss: 3.8451498747 |████████████████----| 82.0%  - val_loss: 1960.2737426758 - val_trvae_loss: 1960.2702636719 - val_landmark_loss: 0.0035211383 - val_unlabeled_loss: 3.5211380720 |████████████████----| 83.0%  - val_loss: 1960.0733032227 - val_trvae_loss: 1960.0698852539 - val_landmark_loss: 0.0034629358 - val_unlabeled_loss: 3.4629356861 |████████████████----| 84.0%  - val_loss: 1961.5155639648 - val_trvae_loss: 1961.5120849609 - val_landmark_loss: 0.0034705367 - val_unlabeled_loss: 3.4705364704 |█████████████████---| 85.0%  - val_loss: 1958.2019042969 - val_trvae_loss: 1958.1983642578 - val_landmark_loss: 0.0035042912 - val_unlabeled_loss: 3.5042910576 |█████████████████---| 86.0%  - val_loss: 1956.9642944336 - val_trvae_loss: 1956.9608154297 - val_landmark_loss: 0.0034962615 - val_unlabeled_loss: 3.4962613583 |█████████████████---| 87.0%  - val_loss: 1955.9615478516 - val_trvae_loss: 1955.9580688477 - val_landmark_loss: 0.0034881911 - val_unlabeled_loss: 3.4881908894 |█████████████████---| 88.0%  - val_loss: 1956.6457519531 - val_trvae_loss: 1956.6417846680 - val_landmark_loss: 0.0039586569 - val_unlabeled_loss: 3.9586566687 |█████████████████---| 89.0%  - val_loss: 1956.4218750000 - val_trvae_loss: 1956.4181518555 - val_landmark_loss: 0.0037063740 - val_unlabeled_loss: 3.7063738108 |██████████████████--| 90.0%  - val_loss: 1953.6693725586 - val_trvae_loss: 1953.6658325195 - val_landmark_loss: 0.0035030131 - val_unlabeled_loss: 3.5030128956 |██████████████████--| 91.0%  - val_loss: 1957.0281372070 - val_trvae_loss: 1957.0245971680 - val_landmark_loss: 0.0034884866 - val_unlabeled_loss: 3.4884864092 |██████████████████--| 92.0%  - val_loss: 1955.5053710938 - val_trvae_loss: 1955.5014038086 - val_landmark_loss: 0.0039643722 - val_unlabeled_loss: 3.9643720388 |██████████████████--| 93.0%  - val_loss: 1953.8837280273 - val_trvae_loss: 1953.8802490234 - val_landmark_loss: 0.0035142486 - val_unlabeled_loss: 3.5142483711 |██████████████████--| 94.0%  - val_loss: 1953.8050537109 - val_trvae_loss: 1953.8016967773 - val_landmark_loss: 0.0033989925 - val_unlabeled_loss: 3.3989923000 |███████████████████-| 95.0%  - val_loss: 1950.4769897461 - val_trvae_loss: 1950.4730834961 - val_landmark_loss: 0.0039450446 - val_unlabeled_loss: 3.9450445175 |███████████████████-| 96.0%  - val_loss: 1954.7355346680 - val_trvae_loss: 1954.7320556641 - val_landmark_loss: 0.0034950656 - val_unlabeled_loss: 3.4950654507 |███████████████████-| 97.0%  - val_loss: 1949.3604125977 - val_trvae_loss: 1949.3568725586 - val_landmark_loss: 0.0035162498 - val_unlabeled_loss: 3.5162496567 |███████████████████-| 98.0%  - val_loss: 1951.4020385742 - val_trvae_loss: 1951.3980712891 - val_landmark_loss: 0.0039599196 - val_unlabeled_loss: 3.9599194527 |███████████████████-| 99.0%  - val_loss: 1951.5836181641 - val_trvae_loss: 1951.5797119141 - val_landmark_loss: 0.0039069344 - val_unlabeled_loss: 3.9069341421 |████████████████████| 100.0%  - val_loss: 1947.9282226562 - val_trvae_loss: 1947.9247436523 - val_landmark_loss: 0.0035104122 - val_unlabeled_loss: 3.5104119778
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y, hue. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
2021-10-26 10:59:49 (INFO): Computing metrics
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/icb/carlo.dedonno/anaconda3/envs/lataq_cuda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
2021-10-26 10:59:50 (INFO): Compute integration metrics
... storing 'study' as categorical
... storing 'cell_type' as categorical
slurmstepd: error: *** JOB 3781263 ON gpusrv13 CANCELLED AT 2021-10-26T11:01:37 ***
